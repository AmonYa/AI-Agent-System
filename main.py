from core.model_loader import load_model
from core.logger import logger

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model, tokenizer = load_model()
logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

def chat():
    """–ß–∞—Ç —Å WhiteRabbitNeo"""
    print("\nüí¨ WhiteRabbitNeo –∑–∞–ø—É—â–µ–Ω. –í–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.")
    while True:
        user_input = input("üßë‚Äçüíª –¢—ã: ")
        if user_input.lower() in ["–≤—ã—Ö–æ–¥", "exit"]:
            print("üëã –ß–∞—Ç –∑–∞–≤–µ—Ä—à–µ–Ω.")
            break

        inputs = tokenizer(user_input, return_tensors="pt").to("cuda")
        output = model.generate(**inputs, max_new_tokens=100)
        response = tokenizer.decode(output[0], skip_special_tokens=True)

        print(f"ü§ñ WhiteRabbitNeo: {response}")
        logger.info(f"–¢—ã: {user_input} | –û—Ç–≤–µ—Ç: {response}")

if __name__ == "__main__":
    chat()
